
# Adversarial Robustness in MNIST Classification

This project investigates the vulnerability of neural networks to adversarial examples using the MNIST dataset. It implements white-box adversarial attacks (FGSM and PGD), visualizes their impact on model performance, and explores adversarial training as a defense strategy.

---

##  Objectives
- Implement FGSM and PGD adversarial attacks.
- Evaluate the robustness of a CNN model on adversarial examples.
- Perform adversarial training using mixed datasets (clean + adversarial).
- Compare pre- and post-defense accuracy across varying epsilon values.
- Visualize model vulnerability and defense effectiveness.

---

## Methodology

### 1. **Model Training**
- A simple CNN was trained on the MNIST dataset to serve as the target model.
- Achieved baseline accuracy: **98.65%**

### 2. **FGSM Attack**
- Fast Gradient Sign Method (FGSM) implemented and applied across the test set.
- Accuracy dropped based on perturbation strength (Îµ):
  - At Îµ = 0.1 â†’ Accuracy: **84.36%**
  - At Îµ = 0.8 â†’ Accuracy: **11.15%**

### 3. **PGD Attack**
- Projected Gradient Descent (PGD) applied for stronger attacks.
- Accuracy dropped drastically:
  - At Îµ = 0.1 â†’ Accuracy: **83.25%**
  - At Îµ = 0.3 â†’ Accuracy: **0.06%**

### 4. **Adversarial Training**
- Model retrained using a dataset composed of:
  - 40% clean images
  - 30% FGSM adversarial images
  - 30% PGD adversarial images
- Improved robustness to adversarial attacks (especially FGSM).

---

## Results Summary

| Attack Type | Epsilon | Accuracy (Before Defense) | Accuracy (After Defense) |
| ----------- | ------- | ------------------------- | ------------------------ |
| FGSM        | 0.1     | 84.36%                    | 96.18%                   |
| FGSM        | 0.8     | 11.15%                    | 36.47%                   |
| PGD         | 0.1     | 83.25%                    | 96.24%                   |
| PGD         | 0.3     | 0.06%                     | 87.19%                   |


---

## Project Structure
â”œâ”€â”€ 1.MNIST_Adversarial_target_cnn_trained.ipynb # CNN model training
â”œâ”€â”€ 2.FGSM_attack.ipynb # FGSM attack implementation 
â”œâ”€â”€ 3.PGD.ipynb # PGD attack implementation 
â”œâ”€â”€ 4.Adversarial_Defense_Model_training_On_adversarial_examples_.ipynb # Defense via adversarial training 
â”œâ”€â”€ mnist_cnn.pth # Baseline model weights 
â”œâ”€â”€ mnist_cnn_adversarially_trained.pth # Robust model weights
â”œâ”€â”€ README.md # Project documentation


---

## ğŸ§© Future Work
- Implement black-box attacks.
- Explore other defenses like:
  - Input preprocessing
  - Label smoothing
  - Ensemble defenses
- Organize results into a single summary notebook for easier sharing and comparison.

---

## ğŸ‘¨â€ğŸ’» Author
**Nikhar Asthana**  
This project was built as part of my learning journey into adversarial machine learning and model robustness.  
Looking for AI/ML research internship opportunities to apply and deepen this knowledge.

---

## ğŸ“œ License
This project is open for educational and research use.
